{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4de07a4",
   "metadata": {
    "papermill": {
     "duration": 0.005869,
     "end_time": "2023-09-24T11:16:55.547556",
     "exception": false,
     "start_time": "2023-09-24T11:16:55.541687",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction  \n",
    "\n",
    "\n",
    "We will use LlaMa v2 model to test if it can be used to perform simple math operations.\n",
    "\n",
    "The model used is:\n",
    "\n",
    "* **Model**: Llama 2  \n",
    "* **Variation**: 7b-chat-hf  \n",
    "* **Version**: V1  \n",
    "* **Framework**: PyTorch  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64711d19",
   "metadata": {
    "papermill": {
     "duration": 0.005046,
     "end_time": "2023-09-24T11:16:55.558256",
     "exception": false,
     "start_time": "2023-09-24T11:16:55.553210",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2aaf8d1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-09-24T11:16:55.571372Z",
     "iopub.status.busy": "2023-09-24T11:16:55.570755Z",
     "iopub.status.idle": "2023-09-24T11:19:52.254160Z",
     "shell.execute_reply": "2023-09-24T11:19:52.252888Z"
    },
    "papermill": {
     "duration": 176.693623,
     "end_time": "2023-09-24T11:19:52.256950",
     "exception": false,
     "start_time": "2023-09-24T11:16:55.563327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xformers\r\n",
      "  Downloading xformers-0.0.21-cp310-cp310-manylinux2014_x86_64.whl (167.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.0/167.0 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xformers) (1.23.5)\r\n",
      "Collecting torch==2.0.1 (from xformers)\r\n",
      "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->xformers) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->xformers) (4.6.3)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->xformers) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->xformers) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->xformers) (3.1.2)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->xformers)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->xformers)\r\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->xformers)\r\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->xformers)\r\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->xformers)\r\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->xformers)\r\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->xformers)\r\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->xformers)\r\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->xformers)\r\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->xformers)\r\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->xformers)\r\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting triton==2.0.0 (from torch==2.0.1->xformers)\r\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->xformers) (68.0.0)\r\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->xformers) (0.40.0)\r\n",
      "Collecting cmake (from triton==2.0.0->torch==2.0.1->xformers)\r\n",
      "  Downloading cmake-3.27.5-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.1/26.1 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting lit (from triton==2.0.0->torch==2.0.1->xformers)\r\n",
      "  Downloading lit-16.0.6.tar.gz (153 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.7/153.7 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.0.1->xformers) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.1->xformers) (1.3.0)\r\n",
      "Building wheels for collected packages: lit\r\n",
      "  Building wheel for lit (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.6-py3-none-any.whl size=93584 sha256=8bd4e4c58a4b8acd8649503969a5ed3bbc1db45f9be1593dea7b769dce434c3d\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/14/f9/07/bb2308587bc2f57158f905a2325f6a89a2befa7437b2d7e137\r\n",
      "Successfully built lit\r\n",
      "Installing collected packages: lit, cmake, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, xformers\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.0.0\r\n",
      "    Uninstalling torch-2.0.0:\r\n",
      "      Successfully uninstalled torch-2.0.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "torchdata 0.6.0 requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed cmake-3.27.5 lit-16.0.6 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 triton-2.0.0 xformers-0.0.21\r\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "import warnings\n",
    "from time import time\n",
    "warnings.filterwarnings('ignore')\n",
    "!pip install xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d91808ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-24T11:19:52.405006Z",
     "iopub.status.busy": "2023-09-24T11:19:52.404264Z",
     "iopub.status.idle": "2023-09-24T11:19:52.412453Z",
     "shell.execute_reply": "2023-09-24T11:19:52.411515Z"
    },
    "papermill": {
     "duration": 0.085229,
     "end_time": "2023-09-24T11:19:52.414449",
     "exception": false,
     "start_time": "2023-09-24T11:19:52.329220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model_tokenize_create_pipeline():\n",
    "    \"\"\"\n",
    "    Load the model\n",
    "    Create a \n",
    "    Args\n",
    "    Returns:\n",
    "        tokenizer\n",
    "        pipeline\n",
    "    \"\"\"\n",
    "    # adapted from https://huggingface.co/blog/llama2#using-transformers\n",
    "    time_1 = time()\n",
    "    model = \"/kaggle/input/llama-2/pytorch/7b-chat-hf/1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    time_2 = time()\n",
    "    print(f\"Load model and init tokenizer: {round(time_2-time_1, 3)}\")\n",
    "    pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",)\n",
    "    time_3 = time()\n",
    "    print(f\"Prepare pipeline: {round(time_3-time_2, 3)}\")\n",
    "    return tokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed8765af",
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2023-09-24T11:19:52.565235Z",
     "iopub.status.busy": "2023-09-24T11:19:52.564899Z",
     "iopub.status.idle": "2023-09-24T11:19:52.570938Z",
     "shell.execute_reply": "2023-09-24T11:19:52.569932Z"
    },
    "papermill": {
     "duration": 0.083669,
     "end_time": "2023-09-24T11:19:52.572837",
     "exception": false,
     "start_time": "2023-09-24T11:19:52.489168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_model(tokenizer, pipeline, prompt_to_test):\n",
    "    \"\"\"\n",
    "    Perform a query\n",
    "    print the result\n",
    "    Args:\n",
    "        tokenizer: the tokenizer\n",
    "        pipeline: the pipeline\n",
    "        prompt_to_test: the prompt\n",
    "    Returns\n",
    "        None\n",
    "    \"\"\"\n",
    "    # adapted from https://huggingface.co/blog/llama2#using-transformers\n",
    "    time_1 = time()\n",
    "    sequences = pipeline(\n",
    "        prompt_to_test,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=200,)\n",
    "    time_2 = time()\n",
    "    print(f\"Test inference: {round(time_2-time_1, 3)}\")\n",
    "    for seq in sequences:\n",
    "        print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896c9a61",
   "metadata": {
    "papermill": {
     "duration": 0.073809,
     "end_time": "2023-09-24T11:19:52.719761",
     "exception": false,
     "start_time": "2023-09-24T11:19:52.645952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Initialize the model, tokenizer and create a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfe6b8ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-24T11:19:52.875130Z",
     "iopub.status.busy": "2023-09-24T11:19:52.873396Z",
     "iopub.status.idle": "2023-09-24T11:23:03.033635Z",
     "shell.execute_reply": "2023-09-24T11:23:03.032668Z"
    },
    "papermill": {
     "duration": 190.239864,
     "end_time": "2023-09-24T11:23:03.035977",
     "exception": false,
     "start_time": "2023-09-24T11:19:52.796113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model and init tokenizer: 0.171\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9f8eb479fe4d24ae729a1929a12486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare pipeline: 189.981\n"
     ]
    }
   ],
   "source": [
    "tokenizer, pipeline = load_model_tokenize_create_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d662167",
   "metadata": {
    "papermill": {
     "duration": 0.074519,
     "end_time": "2023-09-24T11:23:03.182668",
     "exception": false,
     "start_time": "2023-09-24T11:23:03.108149",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After we initialized the model and the tokenizer, we created a pipeline. Creatin of the pipeline takes the longest time.\n",
    "Now let's test the model with few mathematical prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac5dc30",
   "metadata": {
    "papermill": {
     "duration": 0.072314,
     "end_time": "2023-09-24T11:23:03.330603",
     "exception": false,
     "start_time": "2023-09-24T11:23:03.258289",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Can LlaMa v2 do simple math?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5258af",
   "metadata": {
    "papermill": {
     "duration": 0.073853,
     "end_time": "2023-09-24T11:23:03.476286",
     "exception": false,
     "start_time": "2023-09-24T11:23:03.402433",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prompt #1: Perform a simple sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b72bbe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-24T11:23:03.621333Z",
     "iopub.status.busy": "2023-09-24T11:23:03.620946Z",
     "iopub.status.idle": "2023-09-24T11:23:11.356757Z",
     "shell.execute_reply": "2023-09-24T11:23:11.354418Z"
    },
    "papermill": {
     "duration": 7.811591,
     "end_time": "2023-09-24T11:23:11.359100",
     "exception": false,
     "start_time": "2023-09-24T11:23:03.547509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test inference: 7.73\n",
      "Result: Prompt: Adrian has three apples. His sister Anne has ten apples more than him. How many apples has Anne?\n",
      "\n",
      "Answer: If Adrian has 3 apples, and Anne has 10 apples more than him, then Anne has 13 apples (3 + 10 = 13).\n"
     ]
    }
   ],
   "source": [
    "prompt_to_test = 'Prompt: Adrian has three apples. His sister Anne has ten apples more than him. How many apples has Anne?'\n",
    "test_model(tokenizer, pipeline, prompt_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c90c5d",
   "metadata": {
    "papermill": {
     "duration": 0.072369,
     "end_time": "2023-09-24T11:23:11.505484",
     "exception": false,
     "start_time": "2023-09-24T11:23:11.433115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prompt #2: Ask for the area of a circle, giving the radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f03594a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-24T11:23:11.654918Z",
     "iopub.status.busy": "2023-09-24T11:23:11.654557Z",
     "iopub.status.idle": "2023-09-24T11:23:17.845816Z",
     "shell.execute_reply": "2023-09-24T11:23:17.844627Z"
    },
    "papermill": {
     "duration": 6.267298,
     "end_time": "2023-09-24T11:23:17.848055",
     "exception": false,
     "start_time": "2023-09-24T11:23:11.580757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test inference: 6.186\n",
      "Result: Prompt: A circle has the radius 5. What is the area of the circle?\n",
      "Solution: To find the area of a circle, we can use the formula: Area = πr^2, where r is the radius of the circle. In this case, the radius of the circle is 5, so we can substitute this value into the formula: Area = π(5)^2 = 3.14(25) = 78.5. So, the area of the circle is 78.5 square units.\n"
     ]
    }
   ],
   "source": [
    "prompt_to_test = 'Prompt: A circle has the radius 5. What is the area of the circle?'\n",
    "test_model(tokenizer, pipeline, prompt_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e95233",
   "metadata": {
    "papermill": {
     "duration": 0.071904,
     "end_time": "2023-09-24T11:23:17.994131",
     "exception": false,
     "start_time": "2023-09-24T11:23:17.922227",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prompt #3: Calculate an equation with two unknowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91bb6517",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-24T11:23:18.194048Z",
     "iopub.status.busy": "2023-09-24T11:23:18.193681Z",
     "iopub.status.idle": "2023-09-24T11:23:24.556810Z",
     "shell.execute_reply": "2023-09-24T11:23:24.554622Z"
    },
    "papermill": {
     "duration": 6.448171,
     "end_time": "2023-09-24T11:23:24.558953",
     "exception": false,
     "start_time": "2023-09-24T11:23:18.110782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test inference: 6.357\n",
      "Result: Prompt: Anne and Adrian have a total of 10 apples. Anne has 2 apples more than Adrian.How many apples has each of the children Anne and Adrian?\n",
      "Answer: Let's say Adrian has x apples. Since Anne has 2 apples more than Adrian, Anne has 2+x apples.\n",
      "We know that the total number of apples is 10, so we can set up the equation:\n",
      "2 + x = 10\n",
      "\n",
      "Solving for x, we get:\n",
      "x = 8\n",
      "\n",
      "So Adrian has 8 apples and Anne has 10 - 8 = 2 apples.\n"
     ]
    }
   ],
   "source": [
    "prompt_to_test = 'Prompt: Anne and Adrian have a total of 10 apples. Anne has 2 apples more than Adrian.\\\n",
    "How many apples has each of the children Anne and Adrian?'\n",
    "test_model(tokenizer, pipeline, prompt_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318d525f",
   "metadata": {
    "papermill": {
     "duration": 0.075829,
     "end_time": "2023-09-24T11:23:24.708862",
     "exception": false,
     "start_time": "2023-09-24T11:23:24.633033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "\n",
    "After we initialized the model and the tokenizer, which took on GPU T4 x2 at the first run under 200 sec. (this time might be variable), then each prompt only took less than 10 sec. \n",
    "\n",
    "The simple math questions are answered sometime correct, sometime incorrectly. \n",
    "\n",
    "It might be related to the temperature factor (that is not set here explicitly)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 396.99538,
   "end_time": "2023-09-24T11:23:28.661516",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-09-24T11:16:51.666136",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0a671e08d9d64b048e9c98eb6ae9cc44": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4781fbe3143646708671d22b00199336": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "72c4d9e77de0413696d93451b8bb7639": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7e9f8eb479fe4d24ae729a1929a12486": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8109b1aff55040488b290d1c303fde77",
        "IPY_MODEL_d2b0a4a533f84f49af0a0ad1810ca972",
        "IPY_MODEL_fce5ba23731d4ede96ef94455ee103e3"
       ],
       "layout": "IPY_MODEL_72c4d9e77de0413696d93451b8bb7639"
      }
     },
     "8109b1aff55040488b290d1c303fde77": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_82b3c7808970422a99a595b5e81c4e45",
       "placeholder": "​",
       "style": "IPY_MODEL_4781fbe3143646708671d22b00199336",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "82b3c7808970422a99a595b5e81c4e45": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "835a48b9836d490296183bc13f5af0f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c9b9583bacca4c4caab80f8ac4e67a3d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d2b0a4a533f84f49af0a0ad1810ca972": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0a671e08d9d64b048e9c98eb6ae9cc44",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_835a48b9836d490296183bc13f5af0f3",
       "value": 2.0
      }
     },
     "e4ab262012d346f2bcabaa421d8157bf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fce5ba23731d4ede96ef94455ee103e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e4ab262012d346f2bcabaa421d8157bf",
       "placeholder": "​",
       "style": "IPY_MODEL_c9b9583bacca4c4caab80f8ac4e67a3d",
       "value": " 2/2 [02:49&lt;00:00, 77.65s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
